## LLMinator: Run & Test LLMs locally
#### Gradio based tool with integrated chatbot to locally run & test LLMs directly from HuggingFace. 

An easy-to-use tool made with Gradio, LangChain, and Torch.

![image](https://github.com/Aesthisia/LLMinator/assets/91900622/54cc0b3f-c5a8-4470-bcc5-a22e5fd24707)



### ‚ö° Features

- Context-aware Chatbot. 
- Inbuilt code syntax highlighting. 
- Load any LLM repo directly from HuggingFace.
- Supports both CPU & Cuda modes. 

## üöÄ How to use

To use LLMinator, follow these simple steps:

- Clone the LLMinator repository from GitHub.
- Navigate to the directory containing the cloned repository.
- Install the required dependencies by running `pip install -r requirements.txt`.
- Run the LLMinator tool using the command `python webui.py`.
- Access the web interface by opening the provided URL in your browser.
- Start interacting with the chatbot and experimenting with LLMs!

### Command line arguments 

| Argument Command | Default | Description |
| ---------- | ---------- | ---------- |
| --host | 127.0.0.1 | Host or IP address on which the server will listen for incoming connections |
| --port | 7860 | Launch gradio with given server port |
| --share | False | This generates a public shareable link that you can send to anybody |

## ü§ù Contributions

We welcome contributions from the community to enhance LLMinator further. If you'd like to contribute, please follow these guidelines:

- Fork the LLMinator repository on GitHub.
- Create a new branch for your feature or bug fix.
- Test your changes thoroughly.
- Submit a pull request, providing a clear description of the changes you've made.

Reach out to us: info@aesthisia.com
